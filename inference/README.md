# 💻 推理与部署
Tibetan-Llama2为基座模型，不具有指令理解能力，推荐使用Tibetan-Alpaca进行交互。

## 🎉 创建安装环境
```
conda create -n tla python=3.9
pip install -r requirement
```

## 🤖 命令行交互形式
```
python inference/inference_hf.py \
    --base_model path_to__Tibetan-Llama2_or_Tibetan-alpaca_dir \
    --with_prompt \
    --interactive
```

例：
```
python inference/inference_hf.py \
     --base_model /home/rigthub/Tibetan-Llama2-Tibetan-Alpaca/tibetan-Llama2-7B \
     --with_prompt \
     --interactive
```

```
python inference/inference_hf.py \
     --base_model /home/rigthub/Tibetan-Llama2-Tibetan-Alpaca/tibetan-Alpaca-7B \
     --with_prompt \
     --interactive
```

## 📈 web图形界面交互形式
```
python inference/gradio_demo.py \
     --base_model path_to_Tibetan-alpaca_dir
```

例：
```
python inference/gradio_demo.py \
     --base_model  /home/rigthub/Tibetan-Llama2-Tibetan-Alpaca/tibetan-Llama2-7B \
     --tokenizer_path  /home/rigthub/Tibetan-Llama2-Tibetan-Alpaca/tibetan-Llama2-7B
```

```
python inference/gradio_demo.py \
     --base_model  /home/rigthub/Tibetan-Llama2-Tibetan-Alpaca/tibetan-Alpaca-7B \
     --tokenizer_path  /home/rigthub/Tibetan-Llama2-Tibetan-Alpaca/tibetan-Alpaca-7B
```


## 📝 提示词
交互过程中使用以下提示词效果更佳。
  - 摘要生成
```
གཤམ་གྱི་ཚན་པ་འདིའི་ནང་དོན་གཞིར་བཟུང་ནས་ནང་དོན་གནད་བསྡུས་ཞིག་འབྲི་རོགས།
གཤམ་གྱི་ཚན་པ་དེའི་ནང་དོན་གནད་བསྡུས་འབྲི་རོགས།
```
  例：གཤམ་གྱི་ཚན་པ་འདིའི་ནང་དོན་གཞིར་བཟུང་ནས་ནང་དོན་གནད་བསྡུས་ཞིག་འབྲི་རོགས།དབྱེ་ཧྲ་ཞེས་པའི་དྲག་རླུང་གི་ཤུགས་རྐྱེན་ཐེབས་པའི་དབང་གིས་དབྱིན་ཇི་དང་ཨེར་ལན་གྱི་ས་ཁུལ་མང་བོར་ཆར་ཆེན་འབབ་པའི་ནམ་ཟླ་བྱུང་ནས་འགྲིམ་འགྲུལ་ལ་བཀག་རྒྱ་ཐེབས། དབྱིན་ཇིའི་སྨྱན་བྱད་ཀྱིས་བསྒྲགས་པ་ལྟར་ན། ས་དེའི་ཚེས་ཉེར་གཉིས་ཉིན་གྱི་སྔ་དྲོར། དབྱིན་ཇིའི་སུའུ་ཀི་ལན་ས་ཁུལ་གྱི་ལྕགས་ལམ་ཞབས་ཞུ་མཚམས་ཆད་ཡོད། དབྱིན་ཇིའི་གནམ་གཤིས་བརྟགས་དཔྱད་ཅུའུ་ཡིས་དབྱིན་ཇིའི་ས་ཁུལ་མང་ཆེ་བར་རླུང་ཆེན་གཡུག་པའི་ཉེན་བརྡ་སེར་པོ་བཏང་ཡོད་ལ། དབྱིན་ཀི་ལན་དང་ཝེ་ཨར་ཧྲི་ས་ཁུལ་དུ་རླུང་འཚུབ་འབྱུང་ངེས་རེད། དེ་ལས་གཞན་དྲག་རླུང་གི་ཤུགས་རྐྱེན་དབང་གིས་ཨེར་ལན་གྱི་ཏུའུ་པའོ་ལིན་གནམ་གྲུ་ཐང་གིས་ཚེས་ཉེར་གཅིག་ཉིན་གནམ་གྲུ་གཏོང་ཐེངས་བརྒྱ་དང་བཅུ་ལྷག་གཏོང་མཚམས་བཞག་ཡོད་ལ། འགྲུལ་བ་མང་བོ་གནམ་གྲུ་ཐང་དུ་ཁགས་ཡོད།\
  例：གཤམ་གྱི་ཚན་པ་དེའི་ནང་དོན་གནད་བསྡུས་འབྲི་རོགས།དབྱེ་ཧྲ་ཞེས་པའི་དྲག་རླུང་གི་ཤུགས་རྐྱེན་ཐེབས་པའི་དབང་གིས་དབྱིན་ཇི་དང་ཨེར་ལན་གྱི་ས་ཁུལ་མང་བོར་ཆར་ཆེན་འབབ་པའི་ནམ་ཟླ་བྱུང་ནས་འགྲིམ་འགྲུལ་ལ་བཀག་རྒྱ་ཐེབས། དབྱིན་ཇིའི་སྨྱན་བྱད་ཀྱིས་བསྒྲགས་པ་ལྟར་ན། ས་དེའི་ཚེས་ཉེར་གཉིས་ཉིན་གྱི་སྔ་དྲོར། དབྱིན་ཇིའི་སུའུ་ཀི་ལན་ས་ཁུལ་གྱི་ལྕགས་ལམ་ཞབས་ཞུ་མཚམས་ཆད་ཡོད། དབྱིན་ཇིའི་གནམ་གཤིས་བརྟགས་དཔྱད་ཅུའུ་ཡིས་དབྱིན་ཇིའི་ས་ཁུལ་མང་ཆེ་བར་རླུང་ཆེན་གཡུག་པའི་ཉེན་བརྡ་སེར་པོ་བཏང་ཡོད་ལ། དབྱིན་ཀི་ལན་དང་ཝེ་ཨར་ཧྲི་ས་ཁུལ་དུ་རླུང་འཚུབ་འབྱུང་ངེས་རེད། དེ་ལས་གཞན་དྲག་རླུང་གི་ཤུགས་རྐྱེན་དབང་གིས་ཨེར་ལན་གྱི་ཏུའུ་པའོ་ལིན་གནམ་གྲུ་ཐང་གིས་ཚེས་ཉེར་གཅིག་ཉིན་གནམ་གྲུ་གཏོང་ཐེངས་བརྒྱ་དང་བཅུ་ལྷག་གཏོང་མཚམས་བཞག་ཡོད་ལ། འགྲུལ་བ་མང་བོ་གནམ་གྲུ་ཐང་དུ་ཁགས་ཡོད།
  - 新闻生成
```
གོང་གསལ་གྱི་ཁ་བྱང་འདི་དང་འབྲེལ་བ་ཡོད་པའི་རྩོམ་ཞིག་འབྲི་རོགས།
ཁ་བྱང་ནི་"  "ཡིན་པའི་རྩོམ་ཐུང་ཞིག་འབྲི་རོགས།
```
  例：མཚོ་སྔོན་གྱིས་བྱ་ཐབས་སྣ་ཚོགས་སྤྱད་དེ་མཐོ་སློབ་སློབ་ཐོན་སློབ་མའི་ལས་ཞུགས་ལ་ཞབས་འདེགས་ཞུ་བ།གོང་གསལ་གྱི་ཁ་བྱང་འདི་དང་འབྲེལ་བ་ཡོད་པའི་རྩོམ་ཞིག་འབྲི་རོགས།\
  例：ཁ་བྱང་ནི་“མཚོ་སྔོན་གྱིས་བྱ་ཐབས་སྣ་ཚོགས་སྤྱད་དེ་མཐོ་སློབ་སློབ་ཐོན་སློབ་མའི་ལས་ཞུགས་ལ་ཞབས་འདེགས་ཞུ་བ།“ཡིན་པའི་རྩོམ་ཐུང་ཞིག་འབྲི་རོགས།
  - 开放问答\
  例：བོད་ཅེས་པའི་མིང་འདི་ཇི་ལྟར་བྱུང་བ་དང༌། བོད་ཅེས་པའི་མིང་དེར་གོ་དོན་ཡོད་མེད་ཀྱི་འདོད་ཚུལ་མི་འདྲ་བ་གང་དག་ཡོད་དམ།\
  例：ཀྲུང་གོའི་སྲོལ་རྒྱུན་དུས་ཆེན་ཆེན་པོ་བཞི་གང་དག་ཡིན།
  - 时态生成
```
-བྱ་ཚིག་འདིའི་ད་ལྟ་བ་དང་འདས་པ།མ་འོངས་པ།སྐུལ་ཚིག་སོ་སོ་གང་དག་ཡིན།
-བྱ་ཚིག་འདིའི་དུས་གསུམ་ནི་གང་དག་ཡིན།
```
  例：འཁྱམ-བྱ་ཚིག་འདིའི་ད་ལྟ་བ་དང་འདས་པ།མ་འོངས་པ།སྐུལ་ཚིག་སོ་སོ་གང་དག་ཡིན།\
  例：འཁྱམ-བྱ་ཚིག་འདིའི་དུས་གསུམ་ནི་གང་དག་ཡིན།
  - 释义生成
```
-མདུན་གྱི་ཐ་སྙད་འདི་ལ་འགྲེལ་བཤད་རྒྱག་རོགས།
ཐ་སྙད་འདི་ལ་འོས་འཚམ་གྱི་འགྲེལ་བ་བྱོས།
```
  例：དྲན་འཛིན-མདུན་གྱི་ཐ་སྙད་འདི་ལ་འགྲེལ་བཤད་རྒྱག་རོགས།\
  例：དྲན་འཛིན།ཐ་སྙད་འདི་ལ་འོས་འཚམ་གྱི་འགྲེལ་བ་བྱོས།
  - 文本分类
```
ཚན་པ་འདིའི་ནང་དོན་རིགས་གང་ལ་གཏོགས།
```
  例：ཚན་པ་འདིའི་ནང་དོན་རིགས་གང་ལ་གཏོགས།བོད་ཀྱི་སློབ་འབྲིང་དྲ་བ་ཐེངས་འདིའི་འགྲན་བསྡུར་བྱས་པར་བརྒྱུད་རྫོང་ཡོངས་ཀྱི་བོད་ཡིག་དགེ་རྒན་རྣམས་ཀྱིས་སྨྱན་མང་ཁྲིད་ཆས་བཟོ་བའི་སྤྲོ་སེམས་བསྐྱེད་པ་མ་ཟད་ཕྱིས་སུ་སློབ་ཁྲིད་ཀྱི་ཁྲོད་དུ་སྨྱན་མང་ཁྲིད་ཆས་བཀོལ་སྤྱོད་བྱས་ཏེ་སློབ་མའི་སྦྱངས་འབྲས་མཐོར་འདེགས་བྱེད་པར་སྐུལ་མ་ཐེབས་པའོ་༢༠༡༢ལོའི་ཟླ་༡༡ཚེས་༡༧ནས༡༨ཉིན་བར་དཔའ་རིས་བོད་རང་སྐྱོང་རྫོང་སློབ་གསོ་ཅུའུ་ཡིས་གཙོ་སྒྲུབ་བྱེད་པའི་རྫོང་ཡོངས་ཀྱི་སྐད་གཉིས་སློབ་གྲྭའི་བོད་ཡིག་སྨྱན་མང་ཁྲིད་ཆས་འགྲན་བསྡུར་དུས་ལྟར་དུ་དཔའ་རིས་མི་རིགས་སློབ་འབྲིང་དུ་སྤེལ་ཐེངས་འདིའི་འགྲན་བསྡུར་དུ་ཞུགས་པའི་སྨྱན་མང་ཁྲིད་ཆས་༧༠ལྷག་ཙམ་ཡོད་པ་དང་སློབ་ཆུང་དང་སློབ་འབྲིང་ཁག་གཉིས་སུ་བགོས་ཡོད་དཔྱད་བསྡུར་མཁན་རྣམས་ཀྱིས་གཟབ་ནན་ངང་འདེམས་བསལ་བྱས་མཐར་ཕུལ་བྱུང་ཅན་གྱི་སྨྱན་མང་ཁྲིད་ཆས་༡༧ལ་བྱ་དགའ་ཐོབ།



## 🌟 参数说明
- ```--base_model {base_model}``` ：存放HF格式的LLaMA-2模型权重和配置文件的目录
- ```--lora_model {lora_model}``` ：tibetn-Alpaca LoRA解压后文件所在目录。若不提供此参数，若无提供LoRA文件，则只加载--base_model指定的模型
- ```--tokenizer_path {tokenizer_path}```：存放对应tokenizer的目录。若不提供此参数，则其默认值与--lora_model相同；若也未提供--lora_model参数，则其默认值与--base_model相同
- ```--only_cpu```: 仅使用CPU进行推理
- ```--gpus {gpu_ids}```: 指定使用的GPU设备编号，默认为0。如使用多张GPU，以逗号分隔，如0,1,2
- ```--alpha {alpha}```：使用NTK方法拓展上下文长度的系数，可以提升有效上下文长度。默认为1。如果不知道怎么设置，可以保持默认值，或设为"auto"。
- ```--load_in_8bit```或```--load_in_4bit```：使用8bit或4bit方式加载模型
- ```--max_memory```：多轮对话历史中存储的最大序列长度（以token计），默认1024
- ```--use_vllm```：使用vLLM作为LLM后端进行推理
- ```--speculative_sampling```: 使用投机采样加速推理
- ```--draft_base_model {draft_base_model}```: 存放HF格式的LLaMA-2小模型权重和配置文件的目录
- ```--draft_lora_model {draft_lora_model}```: 小模型的LoRA文件目录。若不提供此参数，则只加载--base_model指定的模型
- ```--draft_model_load_in_8bit```或```--draft_model_load_in_4bit```: 使用8bit或4bit方式加载小模型，降低显存占用
- ```--flash_attn```: 使用Flash-Attention加速推理
## 可自行调整参数并测试其生成结果
